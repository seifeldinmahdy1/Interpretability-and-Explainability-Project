# -*- coding: utf-8 -*-
"""Gradient boosting(Ahmed Abouelela).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16QP1DY8kVcqrxiqAhiBF1iWcv_7hChvi
"""

from sklearn.datasets import load_breast_cancer
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import RFECV
import pandas as pd
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.io as pio
import plotly.figure_factory as ff
from sklearn.feature_selection import mutual_info_classif
from sklearn.feature_selection import VarianceThreshold
from sklearn.feature_selection import f_classif
from sklearn.ensemble import GradientBoostingClassifier
pio.renderers.default = 'colab'

!pip install artemis

!pip install pyartemis

"""# Loading Data and **Preprocessing**"""

data = load_breast_cancer()

print(f"Dataset shape: {data.data.shape}")
print(f"Target names: {data.target_names}")

feature_names = data.feature_names
target_names = data.target_names
df = pd.DataFrame(data.data, columns=feature_names)
df['target'] = data.target
print(df.head())

df

print(df.isnull().sum())

X = df.drop('target', axis=1)
y = df['target']

"""# **Standardization**"""

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_scaled = pd.DataFrame(X_scaled, columns=X.columns)

"""# **Splitting**"""

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)

df.shape

len(data.feature_names)

y.value_counts(normalize=True)

X_train.shape

X_test.shape

df.duplicated().sum()

features = df.drop('target', axis=1)

selected_features=['mean smoothness', 'mean compactness', 'mean concavity', 'mean concave points', 'mean symmetry', 'mean fractal dimension', 'radius error', 'perimeter error', 'area error', 'smoothness error', 'compactness error', 'concavity error', 'concave points error', 'symmetry error', 'fractal dimension error', 'worst radius', 'worst texture', 'worst perimeter', 'worst area', 'worst smoothness', 'worst compactness', 'worst concavity', 'worst concave points', 'worst symmetry', 'worst fractal dimension']

X_df = pd.DataFrame(X, columns=data.feature_names)

X_train_df =  pd.DataFrame(X_train, columns=data.feature_names)

"""# **GradiendBoosting**"""

gb = GradientBoostingClassifier()
gb.fit(X_train, y_train)

gb_pred=gb.predict(X_test)

accuracy_score(y_test, gb_pred)

print(classification_report(y_test, gb_pred))

"""# **Confusion Matrix**"""

print("Confusion Matrix:")
conf_matrix = confusion_matrix(y_test, gb_pred)
print(conf_matrix)

labels = ['Benign', 'Malignant']
fig = ff.create_annotated_heatmap(
    z=conf_matrix,
    x=labels,
    y=labels,
    colorscale='Viridis',
    showscale=True,
    annotation_text=[[str(value) for value in row] for row in conf_matrix]
)
fig.update_layout(
    title="Confusion Matrix",
    xaxis_title="Predicted Label",
    yaxis_title="True Label",
    width=600,
    height=600
)
fig.show()

"""- The confusion matrix shows a classification model with 38 true benign predictions and 71 true malignant predictions, with 1 false benign and 4 false malignant cases, indicating high accuracy for malignant predictions but some missclassification for benign cases.

# Learning **Curve**
"""

import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import learning_curve
import numpy as np
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)




train_sizes = [1, 75, 150, 270, 331]


train_sizes, train_scores, test_scores = learning_curve(
    estimator=gb,
    X=X_train_scaled,
    y=y_train,
    cv=5,
    scoring="accuracy",
    train_sizes=train_sizes
)


train_mean = np.mean(train_scores, axis=1)
test_mean = np.mean(test_scores, axis=1)


plt.subplots(figsize=(10, 8))
plt.plot(train_sizes, train_mean, label="Train")
plt.plot(train_sizes, test_mean, label="Validation")


plt.title("Learning Curve for GradientBoosting", fontsize=14)
plt.xlabel("Training Set Size", fontsize=12)
plt.ylabel("Accuracy", fontsize=12)
plt.legend(loc="best")

plt.show()

"""##The GradientBoosting learning curve has the training accuracy  at 1.0, and the validation accuracy rising from 0.90 to around 0.96 and plateus, showing good generalization with minimal underfitting as the validation accuracy has not reached the training accuracy.

# Validation **curve**
"""

from sklearn.model_selection import validation_curve



def plot_validation_curve(estimator, title, X, y, param_name, param_range, cv=5, scoring="accuracy"):
    train_scores, test_scores = validation_curve(
        estimator=estimator,
        X=X,
        y=y,
        param_name=param_name,
        param_range=param_range,
        cv=cv,
        scoring=scoring
    )

    train_mean = np.mean(train_scores, axis=1)
    test_mean = np.mean(test_scores, axis=1)

    plt.figure(figsize=(10, 6))
    plt.plot(param_range, train_mean, label="Training score", color="blue")
    plt.plot(param_range, test_mean, label="Cross-validation score", color="red")

    plt.title(title)
    plt.xlabel(param_name)
    plt.ylabel("Accuracy")
    plt.legend(loc="best")
    plt.grid()
    plt.show()



param_range = np.arange(1, 21)
plot_validation_curve(
    gb,
    "Validation Curve for Gradient boosting",
    X_train_scaled,
    y_train,
    "max_depth",
    param_range,
    cv=5,
    scoring="accuracy"
)

"""## Gradient Boosting validation curve shows that the training score  is constant at 1.0, but the cross-validation score drops from 0.98 to 0.92 with increasing max_depth, which indicates overfitting as tree depth increases.

# Precision-Recall **Curve**
"""

from sklearn.metrics import precision_recall_curve
y_probs_gb = gb.predict_proba(X_test)[:, 1]
precision, recall, thresholds = precision_recall_curve(y_test, y_probs_gb)

plt.figure(figsize=(8, 6))
plt.plot(recall, precision, marker=".", label="GradientBoosting Classifier")
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Precision-Recall Curve")
plt.legend()
plt.grid()
plt.show()

"""##The precision-recall curve for GradientBoosting Classifier shows high precision around 1.0 up to around a recall of around 0.95, followed by a sharp decrease, showing extremely good performance on most positive predictions

# ROC **Curve**
"""

from sklearn.metrics import roc_curve, auc
fpr, tpr, thresholds = roc_curve(y_test, y_probs_gb)
roc_auc = auc(fpr, tpr)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color="darkorange", lw=2, label=f"ROC Curve (AUC = {roc_auc:.2f})")
plt.plot([0, 1], [0, 1], color="navy", lw=2, linestyle="--", label="Random Guess")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Receiver Operating Characteristic (ROC) Curve")
plt.legend(loc="lower right")
plt.grid()
plt.show()

"""##The ROC curve of the GradientBoosting shows a very high Area Under the Curve  of 0.99 with very good performance and a practically perfect True Positive Rate and very low False Positive Rate in comparison to random guess

# **ELI5**
"""

!pip install eli5

import eli5
from eli5.sklearn import PermutationImportance

print("\nELI5 Explanation for GradientBoosting:")
display(eli5.show_weights(gb, feature_names=X.columns.tolist()))

"""##The ELI5 Explanation for GradientBoosting shows worst_radius feature as the most important feature, meaning it has the biggest impact on the model's predictions"""

print("\nELI5 Explanation for GradientBossting with Permutation Importance :")
perm = PermutationImportance(gb, random_state=42).fit(X_test, y_test)
display(eli5.show_weights(perm, feature_names=X.columns.tolist()))

"""##The ELI5 Explanation for GradientBoosting with Permutation Importance indicates worst_concave_points feature as the most important feature for predictions in the model, followed by mean_texture feature, while features like radius_error and mean_smoothness have no effect."""

eli5.show_prediction(gb, X_test.iloc[1], horizontal_layout=False, show_feature_values=True)

"""##The ELI5 explanation shows a GradientBoosting model predicting a 100% probability (score 8.582) of class y=1, with worst_concave_points feature and smoothness_error feature  as top positive contributors

# **PDP**
"""

from sklearn.inspection import PartialDependenceDisplay

feature = [0]

target_class = 0

PartialDependenceDisplay.from_estimator(gb, X_test, features=feature, target=target_class)
plt.show()

"""##The partial dependence plot shows that the model's prediction sharply increases from 1.406 to 1.416 as mean_radius goes up from around -0.5 to 0.5, which indicates a very strong positive correlation between mean radius and the predicted value

# **ICE**
"""

PartialDependenceDisplay.from_estimator(gb, X_test, features=feature, target=target_class, kind='individual')
plt.title("ICE - Gradient Boosting")
plt.show()

"""##The ICE plot of Gradient Boosting shows that single predictions mostly are flat at 0.0 or 1.0 partial dependence along the range of mean_radius, indicating stable prediction behavior with low variability for most of the instances.

# **ALE**
"""

!pip install alibi

from alibi.explainers import ALE, plot_ale

proba_fun_gb = gb.predict_proba

proba_ale_gb = ALE(proba_fun_gb, feature_names=feature_names, target_names=target_names)

proba_exp_gb = proba_ale_gb.explain(X_train.values)

plot_ale(proba_exp_gb, features=selected_features, n_cols=5, fig_kw={'figwidth': 15, 'figheight': 10})

"""##ALE plots show how features affect GradientBoosting predictions, where worst_area and worst_radius have the largest effects, while features like radius_error and symmetry_error do not have an effect

# Permutation Feature **Importance**
"""

from sklearn.inspection import permutation_importance

result = permutation_importance(gb, X_test, y_test, n_repeats=10, random_state=42)
sorted_idx = result.importances_mean.argsort()

plt.barh(X_df.columns[sorted_idx], result.importances_mean[sorted_idx])
plt.title("Permutation Feature Importance - GradientBoosting")
plt.show()

"""##The Permutation Feature Importance for GradientBoosting determines worst_concave_points as the most important feature,followed by worst_texture and mean_texture, while feature like "worst_perimeter" have minimal contribution.

# **LOFO**
"""

from sklearn.model_selection import cross_val_score

baseline = cross_val_score(gb, X_train, y_train, cv=5).mean()

lofo_scores = {}
for col in X_df.columns:
    X_lofo = X_train_df.drop(columns=[col])
    score = cross_val_score(gb, X_lofo, y_train, cv=5).mean()
    lofo_scores[col] = baseline - score

lofo_df = pd.Series(lofo_scores).sort_values(ascending=False)
lofo_df.plot(kind='barh')
plt.title("LOFO Importance - GradientBoosting")
plt.show()

"""##The LOF0 Importance for GradientBoosting shows worst_texture as the most important feature, followed by worst_concavity,while features like mean_concave_points and mean_concavity have a negative importance

# **Global Surrogate**
"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import r2_score

surrogate = DecisionTreeClassifier(max_depth=3)
gb_preds = gb.predict(X_train)
surrogate.fit(X_train, gb_preds)

from sklearn import tree
plt.figure(figsize=(12,6))
tree.plot_tree(surrogate, feature_names=X_df.columns, filled=True)
plt.title("Global Surrogate Tree -  GradientBoosting")
plt.show()

"""##Global Surrogate Tree of GradientBoosting splits primarily based on worst_radius (≤ 0.109), worst_concave_points (≤ 0.325), and texture_error (≤ -1.349), achieving high purity at the leaves gini = 0.0

# **H-statistic**
"""

from artemis.interactions_methods.model_agnostic import FriedmanHStatisticMethod
import random

random.seed(8)

X_exp = random.choices(X_train.values.tolist(), k=100)
X_exp_df = pd.DataFrame(X_exp, columns=feature_names)


X_exp_scaled = scaler.transform(X_exp_df)


X_exp_scaled_df = pd.DataFrame(X_exp_scaled, columns=feature_names)


h_stat = FriedmanHStatisticMethod()
h_stat.fit(gb, X_exp_scaled_df)

# Overall interaction plot
fig, ax = plt.subplots(figsize=(10, 4))
h_stat.plot('bar_chart_ova',ax=ax)

"""##Interaction with the other features chart shows that worst_texture has the highest Friedman H-statistic interaction with the other features in the GradientBoosting model, followed by worst_concave_points and worst_smoothness indicating high feature interactions, while worst_concavity has the lowest interaction"""

# Pairwise interactions
fig, ax = plt.subplots(figsize=(10, 4))
h_stat.plot(vis_type='bar_chart',ax=ax)

"""##Pair interaction chart shows the most significant Friedman H-statistic interaction between worst_radius and area_error and worst_area and area_error, which indicates that these pairs of features significantly impact the predictions of GradientBoosting"""

# Interaction heatmap
h_stat.plot()

!pip install lime

!pip install shap

"""# **LIME**"""

import lime
import lime.lime_tabular

explainer = lime.lime_tabular.LimeTabularExplainer(
    training_data=np.array(X_train_scaled),
    feature_names=feature_names.tolist(),
    class_names=['malignant', 'benign'],
    mode='classification',
    discretize_continuous=True
)

i = 0
exp = explainer.explain_instance(
    data_row=X_test_scaled[i],
    predict_fn=gb.predict_proba,
    num_features=len(feature_names)
)


exp.show_in_notebook(show_table=True, show_all=False)

"""##The LIME model explanation for a GradientBoosting malignant prediction with probability 1.00 indicates the strongest positive contribution from worst_concave_points, with no impacts from features like "mean_concavity  and mean_compactness

# **SHAP**
"""

import shap
background_data = shap.sample(X_train_scaled, 100)


explainer = shap.KernelExplainer(gb.predict_proba, background_data)


shap_values = explainer.shap_values(X_test_scaled)


plt.figure()
shap.summary_plot(shap_values[:, :, 1], X_test_scaled, feature_names=feature_names, show=True)
plt.show()

"""##The SHAP plot of the GradientBoosting model shows worst_area and worst_concave_points as the most important features,while features like compactness_error have smallest effect."""

plt.figure()
shap.summary_plot(shap_values[:, :, 1], X_test_scaled, feature_names=feature_names, plot_type="bar", show=True)
plt.close()

"""##The GradientBoosting SHAP  bar plot shows worst_area and worst_concave_points as the features with the strongest impact , while features like smoothness_error and compactness_error have the least impact,showing their average contribution to predictions."""

explanation = shap.Explanation(
    values=shap_values[:, :, 1],
    base_values=explainer.expected_value[1],
    data=X_test_scaled,
    feature_names=feature_names
)

num_features = len(feature_names)

# Bar plot for global feature importance
plt.figure()
shap.plots.bar(explanation, max_display=num_features, show=True)
plt.close()

"""##The SHAP global feature importance bar plot of  GradientBoosting  shows that worst_area (+0.1) and worst_concave_points (+0.09) are the most important features enhancing the prediction, while mean_radius and mean_symmetry (+0) have no impact."""

plt.figure()
shap.plots.waterfall(explanation[0], max_display=num_features, show=True)
plt.close()

plt.figure()
shap.plots.waterfall(explanation[1], max_display=num_features, show=True)
plt.close()

shap_explainer = shap.Explainer(gb, X_train_scaled)
shap_values = shap_explainer(X_test_scaled)

shap_values1 = shap_values[:, :]

explanation = shap.Explanation(values=shap_values1,
                                 base_values=None,
                                 data=X_test,
                                 feature_names=feature_names)

shap.plots.heatmap(explanation)

"""##The SHAP heatmap plot for the GradientBoosting model shows worst_area and worst_concave_points as the top contributors  to the model's output over instances, with their effects very different from one another, while the combined contribution of 21 other features is smaller"""