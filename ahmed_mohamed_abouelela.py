# -*- coding: utf-8 -*-
"""Ahmed Mohamed Abouelela.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ii6TkFl1tUYdy5UsdS57pAnd7TUGGdTV
"""

from sklearn.datasets import load_breast_cancer
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import RFECV
import pandas as pd
import numpy as np
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.io as pio
from sklearn.feature_selection import mutual_info_classif
from sklearn.feature_selection import VarianceThreshold
from sklearn.feature_selection import f_classif
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import plotly.figure_factory as ff
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import validation_curve
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
import eli5
from eli5.sklearn import PermutationImportance
from IPython.display import display
from sklearn.model_selection import cross_val_score
import lime
import lime.lime_tabular
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import roc_curve, auc
from artemis.interactions_methods.model_agnostic import FriedmanHStatisticMethod
pio.renderers.default = 'colab'

!pip install shap

!pip install alibi

!pip install artemis

!pip install pyartemis

!pip install eli5

!pip install lime

data = load_breast_cancer()

print(f"Dataset shape: {data.data.shape}")
print(f"Target names: {data.target_names}")

feature_names = data.feature_names
df = pd.DataFrame(data.data, columns=feature_names)
df['target'] = data.target
print(df.head())

df

print(df.isnull().sum())

X = df.drop('target', axis=1)
y = df['target']

# 3. Scale the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_scaled = pd.DataFrame(X_scaled, columns=X.columns)

# 4. Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)

# Print dataset info
print("Dataset shape:", df.shape)
print("Features:", len(data.feature_names))
print("Target distribution:\n", y.value_counts(normalize=True))
print("Training set shape:", X_train.shape)
print("Testing set shape:", X_test.shape)

duplicates = df.duplicated().sum()
print("Number of duplicate rows:", duplicates)

features = df.drop('target', axis=1)

# 1. IQR Method for Outlier Detection
def detect_outliers_iqr(data):
    outliers_dict = {}
    for column in data.columns:
        Q1 = data[column].quantile(0.25)
        Q3 = data[column].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        # Identify outliers
        outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)][column]
        outliers_dict[column] = {
            'count': len(outliers),
            'outlier_indices': outliers.index.tolist()
        }
    return outliers_dict

# 2. Z-score Method for Outlier Detection
def detect_outliers_zscore(data, threshold=3):
    outliers_dict = {}
    for column in data.columns:
        z_scores = np.abs((data[column] - data[column].mean()) / data[column].std())
        outliers = data[z_scores > threshold][column]
        outliers_dict[column] = {
            'count': len(outliers),
            'outlier_indices': outliers.index.tolist()
        }
    return outliers_dict

# Detect outliers using both methods
print("=== Outlier Detection Results ===")


print("\nIQR Method:")
iqr_outliers = detect_outliers_iqr(features)
total_iqr_outliers = sum(info['count'] for info in iqr_outliers.values())

print(f"\nTotal IQR outliers: {total_iqr_outliers}")

print("\nZ-score Method (threshold = 3):")
zscore_outliers = detect_outliers_zscore(features, threshold=3)
total_zscore_outliers = sum(info['count'] for info in zscore_outliers.values())

print(f"\nTotal Z-score outliers: {total_zscore_outliers}")

"""Since this is a diagnostic dataset, it would be better to keep the outliers as they may be clinically meaningful, and malignant tumors often have more extreme feature values. example: mean radius, worst area, etc."""

# Check data types of all columns
print("Data Types in the Wisconsin Breast Cancer Dataset:")
print(df.dtypes)

# Summarize unique data types
print("\nUnique Data Types:")
print(df.dtypes.value_counts())

"""# There is no need to convert data types as all models handle int64 and float64 types, and there are no object types"""

print("=== Key Statistics ===")
stats = features.agg(['mean', 'std', 'min', 'max']).T.round(2)
print(stats.head(6))  # Show first 6 features

# 2. Concise Skewness Summary
print("\n=== Skewness Summary ===")
print(f"Mean Skewness: {features.skew().mean():.2f}, Range: {features.skew().min():.2f} to {features.skew().max():.2f}")

# 3. Plotly Histograms for 6 Representative Features (Inline Display)
selected_features = ['mean radius', 'mean texture', 'mean perimeter', 'mean area',
                    'mean smoothness', 'worst area']
fig = make_subplots(rows=2, cols=3, subplot_titles=selected_features)
for i, col in enumerate(selected_features):
    row = i // 3 + 1
    col_pos = i % 3 + 1
    fig.add_trace(
        go.Histogram(x=features[col], nbinsx=20, name=col, showlegend=False),
        row=row, col=col_pos
    )
fig.update_layout(
    height=600, width=900, title_text="Feature Distributions",
    title_font_size=14, bargap=0.1
)
fig.update_xaxes(showgrid=False)
fig.update_yaxes(showgrid=False, title_text="")
fig.show()

print("\n=== Target Value Distribution ===")
target_counts = df['target'].value_counts()
target_percent = df['target'].value_counts(normalize=True) * 100
dist_df = pd.DataFrame({
    'Count': target_counts,
    'Percentage': target_percent.round(2)
})
dist_df.index = ['Benign (1)', 'Malignant (0)']
print(dist_df)

# 5. Plotly Bar Plot for Target Distribution (Inline Display)
fig_target = go.Figure(data=[
    go.Bar(
        x=['Benign (1)', 'Malignant (0)'],
        y=target_counts.values,
        marker_color=['#FF9999', '#66CC99']
    )
])
fig_target.update_layout(
    height=400, width=600, title_text="Target Distribution",
    title_font_size=14, xaxis_title="", yaxis_title="Count",
    bargap=0.2
)
fig_target.show()

"""# **KNN Model**"""

knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy of  KNN : {accuracy:.2f}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

"""# **Confusion Matrix**"""

print("Confusion Matrix:")
conf_matrix = confusion_matrix(y_test, y_pred)
print(conf_matrix)

labels = ['Benign', 'Malignant']
fig = ff.create_annotated_heatmap(
    z=conf_matrix,
    x=labels,
    y=labels,
    colorscale='Viridis',
    showscale=True,
    annotation_text=[[str(value) for value in row] for row in conf_matrix]
)
fig.update_layout(
    title="Confusion Matrix",
    xaxis_title="Predicted Label",
    yaxis_title="True Label",
    width=600,
    height=600
)
fig.show()

"""# confusion matrix plot shows the counts of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN).

# **Learning Curve**
"""

import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import learning_curve
import numpy as np
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)


knn = KNeighborsClassifier(n_neighbors=1)


train_sizes = [1, 75, 150, 270, 331]


train_sizes, train_scores, test_scores = learning_curve(
    estimator=knn,
    X=X_train_scaled,
    y=y_train,
    cv=5,
    scoring="accuracy",
    train_sizes=train_sizes
)


train_mean = np.mean(train_scores, axis=1)
test_mean = np.mean(test_scores, axis=1)


plt.subplots(figsize=(10, 8))
plt.plot(train_sizes, train_mean, label="Train")
plt.plot(train_sizes, test_mean, label="Validation")


plt.title("Learning Curve for KNN", fontsize=14)
plt.xlabel("Training Set Size", fontsize=12)
plt.ylabel("Accuracy", fontsize=12)
plt.legend(loc="best")

plt.show()

"""# Validation **Curve**"""

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import validation_curve


scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)


def plot_validation_curve(estimator, title, X, y, param_name, param_range, cv=5, scoring="accuracy"):
    train_scores, test_scores = validation_curve(
        estimator=estimator,
        X=X,
        y=y,
        param_name=param_name,
        param_range=param_range,
        cv=cv,
        scoring=scoring
    )

    train_mean = np.mean(train_scores, axis=1)
    test_mean = np.mean(test_scores, axis=1)

    plt.figure(figsize=(10, 6))
    plt.plot(param_range, train_mean, label="Training score", color="blue")
    plt.plot(param_range, test_mean, label="Cross-validation score", color="red")

    plt.title(title)
    plt.xlabel(param_name)
    plt.ylabel("Accuracy")
    plt.legend(loc="best")
    plt.grid()
    plt.show()

# KNN Classifier
knn = KNeighborsClassifier()
param_range = np.arange(1, 21)
plot_validation_curve(
    knn,
    "Validation Curve for KNN Classifier ",
    X_train_scaled,
    y_train,
    "n_neighbors",
    param_range,
    cv=5,
    scoring="accuracy"
)

"""# Precision-Recall **Curve**"""

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train_scaled, y_train)
y_probs_knn = knn.predict_proba(X_test_scaled)[:, 1]

precision, recall, thresholds = precision_recall_curve(y_test, y_probs_knn)

plt.figure(figsize=(8, 6))
plt.plot(recall, precision, marker=".", label="KNN Classifier")
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Precision-Recall Curve")
plt.legend()
plt.grid()
plt.show()

"""# **(ROC) Curve**"""

import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import roc_curve, auc
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)


knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train_scaled, y_train)


y_probs_knn = knn.predict_proba(X_test_scaled)[:, 1]

fpr, tpr, thresholds = roc_curve(y_test, y_probs_knn)
roc_auc = auc(fpr, tpr)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color="darkorange", lw=2, label=f"ROC Curve (AUC = {roc_auc:.2f})")
plt.plot([0, 1], [0, 1], color="navy", lw=2, linestyle="--", label="Random Guess")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Receiver Operating Characteristic (ROC) Curve")
plt.legend(loc="lower right")
plt.grid()
plt.show()

import eli5
from eli5.sklearn import PermutationImportance

import eli5
from eli5.sklearn import PermutationImportance
from IPython.display import display
import pandas as pd


feature_names = data.feature_names
X_train_df = pd.DataFrame(X_train, columns=feature_names)
X_test_df = pd.DataFrame(X_test, columns=feature_names)


scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_df)
X_test_scaled = scaler.transform(X_test_df)


knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train_scaled, y_train)


y_pred_knn = knn.predict(X_test_scaled)
accuracy_knn = accuracy_score(y_test, y_pred_knn)
print(f'KNN Classifier Accuracy: {accuracy_knn:.2f}')


print("\nELI5 Explanation for KNN Classifier:")
perm = PermutationImportance(knn, random_state=42).fit(X_test_scaled, y_test)
display(eli5.show_weights(perm, feature_names=feature_names.tolist()))  # Display properly

y_test.iloc[0]

X_test.iloc[0]

from sklearn.inspection import PartialDependenceDisplay
features =[0,1,2]
target_class = 0
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, y_train)
PartialDependenceDisplay.from_estimator(
    knn,
    X_test,
    features=features,
    target=target_class,
    feature_names=data.feature_names,
    grid_resolution=50,
    n_cols=3
)


plt.tight_layout()


plt.show()

"""# **LOFO**"""

feature_names = data.feature_names
X_train_df = pd.DataFrame(X_train, columns=feature_names)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_df)
knn = KNeighborsClassifier(n_neighbors=3)
baseline = cross_val_score(knn, X_train_scaled, y_train, cv=5, scoring='accuracy').mean()
lofo_scores = {}
for col in X_train_df.columns:
    X_lofo = X_train_df.drop(columns=[col])
    X_lofo_scaled = scaler.fit_transform(X_lofo)
    score = cross_val_score(knn, X_lofo_scaled, y_train, cv=5, scoring='accuracy').mean()
    lofo_scores[col] = baseline - score
lofo_df = pd.Series(lofo_scores).sort_values(ascending=False)


lofo_df.plot(kind='barh')
plt.title("LOFO Importance - KNN Classifier")
plt.xlabel("Accuracy Drop")
plt.ylabel("Feature")
plt.show()

import random
import pandas as pd
import numpy as np
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from artemis.interactions_methods.model_agnostic import FriedmanHStatisticMethod

random.seed(8)

X_exp = random.choices(X_train.values.tolist(), k=100)
X_exp_df = pd.DataFrame(X_exp, columns=feature_names)


X_exp_scaled = scaler.transform(X_exp_df)


X_exp_scaled_df = pd.DataFrame(X_exp_scaled, columns=feature_names)


h_stat = FriedmanHStatisticMethod()
h_stat.fit(knn, X_exp_scaled_df)

# Overall interaction plot
fig, ax = plt.subplots(figsize=(10, 4))
h_stat.plot('bar_chart_ova',ax=ax)

# Pairwise interactions
fig, ax = plt.subplots(figsize=(10, 4))
h_stat.plot(vis_type='bar_chart',ax=ax)

# Interaction heatmap
h_stat.plot()

"""# **LIME**"""

explainer = lime.lime_tabular.LimeTabularExplainer(
    training_data=np.array(X_train_scaled),
    feature_names=feature_names.tolist(),
    class_names=['malignant', 'benign'],
    mode='classification',
    discretize_continuous=True
)

i = 0
exp = explainer.explain_instance(
    data_row=X_test_scaled[i],
    predict_fn=knn.predict_proba,
    num_features=len(feature_names)
)


exp.show_in_notebook(show_table=True, show_all=False)

import shap

background_data = shap.sample(X_train_scaled, 100)

# Set up SHAP KernelExplainer
explainer = shap.KernelExplainer(knn.predict_proba, background_data)

# Compute SHAP values for the test set
shap_values = explainer.shap_values(X_test_scaled)

# Summary plot (beeswarm) for class 1 (benign)
plt.figure()
shap.summary_plot(shap_values[:, :, 1], X_test_scaled, feature_names=feature_names, show=True)
plt.show()

plt.figure()
shap.summary_plot(shap_values[:, :, 1], X_test_scaled, feature_names=feature_names, plot_type="bar", show=True)
plt.close()

explanation = shap.Explanation(
    values=shap_values[:, :, 1],
    base_values=explainer.expected_value[1],
    data=X_test_scaled,
    feature_names=feature_names
)

num_features = len(feature_names)

# Bar plot for global feature importance
plt.figure()
shap.plots.bar(explanation, max_display=num_features, show=True)
plt.close()

# Waterfall plot for the first test sample
plt.figure()
shap.plots.waterfall(explanation[0], max_display=num_features, show=True)
plt.close()

# Waterfall plot for the second test sample
plt.figure()
shap.plots.waterfall(explanation[1], max_display=num_features, show=True)
plt.close()

# Heatmap of SHAP values across test samples
plt.figure()
shap.plots.heatmap(explanation, max_display=num_features, show=True)
plt.close()

num_features = len(feature_names)

# Bar plot for global feature importance
plt.figure()
shap.plots.bar(explanation, max_display=num_features, show=True)
plt.close()

plt.figure()
shap.plots.waterfall(explanation[0], max_display=num_features, show=True)
plt.close()

plt.figure()
shap.plots.waterfall(explanation[1], max_display=num_features, show=True)
plt.close()

plt.figure()
shap.plots.heatmap(explanation, max_display=num_features, show=True)
plt.close()