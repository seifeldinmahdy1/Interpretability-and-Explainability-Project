# -*- coding: utf-8 -*-
"""Random Forest.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GhX2scL-m-1l1JQK1rfLC-82XBGkD64V
"""

from sklearn.datasets import load_breast_cancer
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix

data = load_breast_cancer()

feature_names = data.feature_names
df = pd.DataFrame(data.data, columns=feature_names)
df['target'] = data.target
df.head()

drop_list1 = ['target']
x_1 = df.drop(drop_list1,axis = 1)

y = df['target']

X_train, X_test, y_train, y_test = train_test_split(x_1, y, test_size=0.3, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

rf_model = RandomForestClassifier(n_estimators=100, criterion='gini',  random_state=42)

rf_model.fit(X_train_scaled, y_train)

rf_predictions = rf_model.predict(X_test_scaled)

def evaluate_model(y_true, y_pred, model_name):
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    print(f"\n{model_name} Performance:")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1-Score: {f1:.4f}")
    print("\nClassification Report:")
    print(classification_report(y_true, y_pred, target_names=['Benign', 'Malignant']))

evaluate_model(y_test, rf_predictions, "Random Forest")

feature_importance = pd.DataFrame({
    'Feature': x_1.columns,
    'Importance': rf_model.feature_importances_
}).sort_values(by='Importance', ascending=False)

print("\nTop 5 Most Important Features (Random Forest):")
print(feature_importance.head())

"""# Validation Curve"""

from sklearn.model_selection import validation_curve

param_range = [10, 50, 100, 150, 200]
train_scores, test_scores = validation_curve(
    RandomForestClassifier(random_state=42),
    X_train_scaled, y_train,
    param_name="n_estimators",
    param_range=param_range,
    cv=5,
    scoring="accuracy",
    n_jobs=-1
)

train_mean = train_scores.mean(axis=1)
test_mean = test_scores.mean(axis=1)

plt.figure(figsize=(8, 5))
plt.plot(param_range, train_mean, label="Train")
plt.plot(param_range, test_mean, label="Validation")
plt.xlabel("n_estimators")
plt.ylabel("Accuracy")
plt.title("Validation Curve (Random Forest)")
plt.legend()
plt.show()

"""# Learning Curve"""

from sklearn.model_selection import learning_curve

train_sizes, train_scores, test_scores = learning_curve(
    rf_model, X_train_scaled, y_train, cv=5,
    train_sizes=np.linspace(0.1, 1.0, 10), scoring='accuracy', n_jobs=-1
)

plt.figure(figsize=(10, 6))
plt.plot(train_sizes, train_scores.mean(axis=1), label='Training Score')
plt.plot(train_sizes, test_scores.mean(axis=1), label='Cross-Validation Score')
plt.fill_between(train_sizes, train_scores.mean(axis=1) - train_scores.std(axis=1),
                 train_scores.mean(axis=1) + train_scores.std(axis=1), alpha=0.1)
plt.fill_between(train_sizes, test_scores.mean(axis=1) - test_scores.std(axis=1),
                 test_scores.mean(axis=1) + test_scores.std(axis=1), alpha=0.1)
plt.title("Learning Curve (Random Forest)")
plt.xlabel("Training Examples")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

"""# Confusion Matrix"""

from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test, rf_predictions)
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=['Benign', 'Malignant'], yticklabels=['Benign', 'Malignant'])
plt.title("Confusion Matrix (Random Forest)")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""# ROC and AUC Curve"""

from sklearn.metrics import roc_curve, auc

rf_probs = rf_model.predict_proba(X_test_scaled)[:, 1]
fpr, tpr, _ = roc_curve(y_test, rf_probs)
roc_auc = auc(fpr, tpr)

plt.figure()
plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('FPR')
plt.ylabel('TPR')
plt.title('ROC Curve (Random Forest)')
plt.legend()
plt.show()

"""# Precision-Recall Curve"""

from sklearn.metrics import precision_recall_curve

precision, recall, _ = precision_recall_curve(y_test, rf_probs)

plt.plot(recall, precision)
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Precision-Recall Curve (Random Forest)")
plt.show()

"""# Permutation Importance"""

from sklearn.inspection import permutation_importance

perm_result = permutation_importance(rf_model, X_test_scaled, y_test, n_repeats=10, random_state=42)
sorted_idx = perm_result.importances_mean.argsort()

plt.figure(figsize=(10, 6))
plt.barh(np.array(feature_names)[sorted_idx], perm_result.importances_mean[sorted_idx])
plt.title("Permutation Importance (Random Forest)")
plt.xlabel("Mean Importance Decrease")
plt.show()

"""# Random Forest Tree Visualization (3 trees)"""

from sklearn.tree import plot_tree

plt.figure(figsize=(24, 30))

for i in range(3):
    plt.subplot(3, 1, i + 1)
    plot_tree(rf_model.estimators_[i],
              feature_names=feature_names,
              class_names=['Benign', 'Malignant'],
              filled=True,
              rounded=True,
              fontsize=6)
    plt.title(f"Random Forest - Tree {i+1}")

plt.tight_layout()
plt.show()

"""# Average Decrease in Impurity (Gini/Entropy)"""

impurity_importance = rf_model.feature_importances_

sorted_idx = np.argsort(impurity_importance)[::-1]
sorted_importances = impurity_importance[sorted_idx]
sorted_features = feature_names[sorted_idx]

plt.figure(figsize=(12, 8))
sns.barplot(x=sorted_importances, y=sorted_features)
plt.title("Mean Decrease in Impurity (Random Forest)")
plt.xlabel("Importance Score")
plt.ylabel("Feature")
plt.show()

"""# ELI5"""

!pip install eli5

import eli5
from eli5.sklearn import PermutationImportance

perm = PermutationImportance(rf_model, random_state=42).fit(X_test_scaled, y_test)
explanation = eli5.explain_weights(perm, feature_names=feature_names)

eli5.show_weights(perm, feature_names=feature_names)

"""# LIME"""

!pip install lime

import lime
import lime.lime_tabular

lime_explainer = lime.lime_tabular.LimeTabularExplainer(
    training_data=X_train_scaled,
    feature_names=feature_names,
    class_names=['Benign', 'Malignant'],
    mode='classification'
)

i = 1
lime_exp = lime_explainer.explain_instance(X_test_scaled[i], rf_model.predict_proba, num_features=10)
lime_exp.show_in_notebook(show_table=True)

"""# SHAP"""

import shap

shap_explainer = shap.Explainer(rf_model, X_train_scaled)
shap_values = shap_explainer(X_test_scaled)

plt.figure()
shap.summary_plot(shap_values[:, :, 1], X_test, feature_names=feature_names, show=True)
plt.show()

plt.figure()
shap.summary_plot(shap_values[:, :, 1], X_test, feature_names=feature_names, plot_type="bar", show=True)
plt.close()

shap_values_class_1 = shap_values[:, :, 1]

explanation = shap.Explanation(values=shap_values_class_1,
                                 base_values=None,
                                 data=X_test,
                                 feature_names=feature_names)

shap.plots.heatmap(explanation)
